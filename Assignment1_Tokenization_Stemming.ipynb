{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6044e0",
   "metadata": {},
   "source": [
    "# Assignment 1: Tokenization and Stemming\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Tokenization**: Whitespace, Punctuation-based, Treebank, Tweet, MWE\n",
    "- **Stemming**: Porter Stemmer, Snowball Stemmer\n",
    "- **Lemmatization**: WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4f1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import (\n",
    "    WhitespaceTokenizer,\n",
    "    WordPunctTokenizer,\n",
    "    TreebankWordTokenizer,\n",
    "    TweetTokenizer,\n",
    "    MWETokenizer\n",
    ")\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acad96f",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d0c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The quick brown foxes are jumping over the lazy dogs! @user #NLP isn't that amazing? :) I'm loving it.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The quick brown foxes are jumping over the lazy dogs! @user #NLP isn't that amazing? :) I'm loving it.\"\n",
    "print(f\"Original Text: {sample_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04770d",
   "metadata": {},
   "source": [
    "## 1. Tokenization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96ebb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. WHITESPACE TOKENIZER:\n",
      "   Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs!', '@user', '#NLP', \"isn't\", 'that', 'amazing?', ':)', \"I'm\", 'loving', 'it.']\n",
      "\n",
      "2. PUNCTUATION-BASED TOKENIZER:\n",
      "   Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '!', '@', 'user', '#', 'NLP', 'isn', \"'\", 't', 'that', 'amazing', '?', ':)', 'I', \"'\", 'm', 'loving', 'it', '.']\n",
      "\n",
      "3. TREEBANK TOKENIZER:\n",
      "   Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '!', '@', 'user', '#', 'NLP', 'is', \"n't\", 'that', 'amazing', '?', ':', ')', 'I', \"'m\", 'loving', 'it', '.']\n",
      "\n",
      "4. TWEET TOKENIZER:\n",
      "   Tokens: ['the', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '!', '@user', '#nlp', \"isn't\", 'that', 'amazing', '?', ':)', \"i'm\", 'loving', 'it', '.']\n",
      "\n",
      "5. MULTI-WORD EXPRESSION (MWE) TOKENIZER:\n",
      "   Base Tokens: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', '!', '@', 'user', '#', 'NLP', 'is', \"n't\", 'that', 'amazing', '?', ':', ')', 'I', \"'m\", 'loving', 'it', '.']\n",
      "   MWE Tokens: ['The', 'quick_brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy_dogs', '!', '@', 'user', '#', 'NLP', 'is', \"n't\", 'that', 'amazing', '?', ':', ')', 'I', \"'m\", 'loving', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# 1. Whitespace Tokenizer\n",
    "print(\"1. WHITESPACE TOKENIZER:\")\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(sample_text)\n",
    "print(f\"   Tokens: {whitespace_tokens}\")\n",
    "\n",
    "# 2. Punctuation-based Tokenizer (WordPunctTokenizer)\n",
    "print(\"\\n2. PUNCTUATION-BASED TOKENIZER:\")\n",
    "punct_tokenizer = WordPunctTokenizer()\n",
    "punct_tokens = punct_tokenizer.tokenize(sample_text)\n",
    "print(f\"   Tokens: {punct_tokens}\")\n",
    "\n",
    "# 3. Treebank Tokenizer\n",
    "print(\"\\n3. TREEBANK TOKENIZER:\")\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(sample_text)\n",
    "print(f\"   Tokens: {treebank_tokens}\")\n",
    "\n",
    "# 4. Tweet Tokenizer\n",
    "print(\"\\n4. TWEET TOKENIZER:\")\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=False, reduce_len=True)\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sample_text)\n",
    "print(f\"   Tokens: {tweet_tokens}\")\n",
    "\n",
    "# 5. Multi-Word Expression (MWE) Tokenizer\n",
    "print(\"\\n5. MULTI-WORD EXPRESSION (MWE) TOKENIZER:\")\n",
    "mwe_tokenizer = MWETokenizer([('quick', 'brown'), ('lazy', 'dogs')])\n",
    "base_tokens = nltk.word_tokenize(sample_text)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(base_tokens)\n",
    "print(f\"   Base Tokens: {base_tokens}\")\n",
    "print(f\"   MWE Tokens: {mwe_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8de484",
   "metadata": {},
   "source": [
    "## 2. Stemming Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5aec0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PORTER STEMMER:\n",
      "   Word -> Stem:\n",
      "   running         -> run\n",
      "   runs            -> run\n",
      "   ran             -> ran\n",
      "   easily          -> easili\n",
      "   fairly          -> fairli\n",
      "   jumping         -> jump\n",
      "   foxes           -> fox\n",
      "   loving          -> love\n",
      "   amazing         -> amaz\n",
      "\n",
      "2. SNOWBALL STEMMER:\n",
      "   Word -> Stem:\n",
      "   running         -> run\n",
      "   runs            -> run\n",
      "   ran             -> ran\n",
      "   easily          -> easili\n",
      "   fairly          -> fair\n",
      "   jumping         -> jump\n",
      "   foxes           -> fox\n",
      "   loving          -> love\n",
      "   amazing         -> amaz\n"
     ]
    }
   ],
   "source": [
    "# Words to stem\n",
    "words_to_stem = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\", \"jumping\", \"foxes\", \"loving\", \"amazing\"]\n",
    "\n",
    "# 1. Porter Stemmer\n",
    "print(\"1. PORTER STEMMER:\")\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stems = [(word, porter_stemmer.stem(word)) for word in words_to_stem]\n",
    "print(\"   Word -> Stem:\")\n",
    "for word, stem in porter_stems:\n",
    "    print(f\"   {word:15} -> {stem}\")\n",
    "\n",
    "# 2. Snowball Stemmer (English)\n",
    "print(\"\\n2. SNOWBALL STEMMER:\")\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "snowball_stems = [(word, snowball_stemmer.stem(word)) for word in words_to_stem]\n",
    "print(\"   Word -> Stem:\")\n",
    "for word, stem in snowball_stems:\n",
    "    print(f\"   {word:15} -> {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6c328",
   "metadata": {},
   "source": [
    "## 3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132d5a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDNET LEMMATIZER:\n",
      "   Word -> Lemma (as noun, verb, adjective):\n",
      "   running         -> n: running      v: run          a: running\n",
      "   runs            -> n: run          v: run          a: runs\n",
      "   ran             -> n: ran          v: run          a: ran\n",
      "   better          -> n: better       v: better       a: good\n",
      "   foxes           -> n: fox          v: fox          a: foxes\n",
      "   geese           -> n: goose        v: geese        a: geese\n",
      "   loving          -> n: loving       v: love         a: loving\n",
      "   amazing         -> n: amazing      v: amaze        a: amazing\n",
      "   dogs            -> n: dog          v: dog          a: dogs\n"
     ]
    }
   ],
   "source": [
    "# WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_to_lemmatize = [\"running\", \"runs\", \"ran\", \"better\", \"foxes\", \"geese\", \"loving\", \"amazing\", \"dogs\"]\n",
    "\n",
    "print(\"WORDNET LEMMATIZER:\")\n",
    "print(\"   Word -> Lemma (as noun, verb, adjective):\")\n",
    "for word in words_to_lemmatize:\n",
    "    lemma_n = lemmatizer.lemmatize(word, pos='n')  # noun\n",
    "    lemma_v = lemmatizer.lemmatize(word, pos='v')  # verb\n",
    "    lemma_a = lemmatizer.lemmatize(word, pos='a')  # adjective\n",
    "    print(f\"   {word:15} -> n: {lemma_n:12} v: {lemma_v:12} a: {lemma_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76679be",
   "metadata": {},
   "source": [
    "## 4. Comparison: Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90bc828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Porter          Snowball        Lemma(v)       \n",
      "------------------------------------------------------------\n",
      "running         run             run             run            \n",
      "better          better          better          better         \n",
      "foxes           fox             fox             fox            \n",
      "studies         studi           studi           study          \n",
      "feet            feet            feet            feet           \n"
     ]
    }
   ],
   "source": [
    "comparison_words = [\"running\", \"better\", \"foxes\", \"studies\", \"feet\"]\n",
    "print(f\"{'Word':<15} {'Porter':<15} {'Snowball':<15} {'Lemma(v)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for word in comparison_words:\n",
    "    porter = porter_stemmer.stem(word)\n",
    "    snowball = snowball_stemmer.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    print(f\"{word:<15} {porter:<15} {snowball:<15} {lemma:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
