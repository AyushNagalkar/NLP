{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f13fba2",
   "metadata": {},
   "source": [
    "# Assignment 2: Bag-of-Words, TF-IDF, and Word2Vec\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Bag-of-Words**: Count occurrence, Normalized count occurrence\n",
    "- **TF-IDF**: Term Frequency-Inverse Document Frequency\n",
    "- **Word2Vec**: Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c50e8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6579beae",
   "metadata": {},
   "source": [
    "## Sample Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89408b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS:\n",
      "  Doc 1: Natural language processing is a field of artificial intelligence.\n",
      "  Doc 2: Machine learning and deep learning are subfields of artificial intelligence.\n",
      "  Doc 3: Text processing involves tokenization and stemming.\n",
      "  Doc 4: Word embeddings capture semantic meaning of words.\n",
      "  Doc 5: NLP applications include sentiment analysis and machine translation.\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Natural language processing is a field of artificial intelligence.\",\n",
    "    \"Machine learning and deep learning are subfields of artificial intelligence.\",\n",
    "    \"Text processing involves tokenization and stemming.\",\n",
    "    \"Word embeddings capture semantic meaning of words.\",\n",
    "    \"NLP applications include sentiment analysis and machine translation.\"\n",
    "]\n",
    "\n",
    "print(\"CORPUS:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"  Doc {i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f5298",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words: Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2f6856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "  ['analysis', 'and', 'applications', 'are', 'artificial', 'capture', 'deep', 'embeddings', 'field', 'include', 'intelligence', 'involves', 'is', 'language', 'learning', 'machine', 'meaning', 'natural', 'nlp', 'of', 'processing', 'semantic', 'sentiment', 'stemming', 'subfields', 'text', 'tokenization', 'translation', 'word', 'words']\n",
      "\n",
      "Count Occurrence Matrix:\n",
      "  Shape: (5, 30)\n",
      "\n",
      "  Document-Term Matrix:\n",
      "  Doc 1: [0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "  Doc 2: [0 1 0 1 1 0 1 0 0 0 1 0 0 0 2 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      "  Doc 3: [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0]\n",
      "  Doc 4: [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1]\n",
      "  Doc 5: [1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "vocabulary = count_vectorizer.get_feature_names_out()\n",
    "print(f\"  {list(vocabulary)}\")\n",
    "\n",
    "print(\"\\nCount Occurrence Matrix:\")\n",
    "count_array = count_matrix.toarray()\n",
    "print(f\"  Shape: {count_array.shape}\")\n",
    "print(\"\\n  Document-Term Matrix:\")\n",
    "for i, row in enumerate(count_array):\n",
    "    print(f\"  Doc {i+1}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b705be",
   "metadata": {},
   "source": [
    "## 2. Bag-of-Words: Normalized Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418fbfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Count Occurrence Matrix:\n",
      "  Shape: (5, 30)\n",
      "\n",
      "  Document-Term Matrix (Normalized):\n",
      "  Doc 1: [0.    0.    0.    0.    0.125 0.    0.    0.    0.125 0.    0.125 0.\n",
      " 0.125 0.125 0.    0.    0.    0.125 0.    0.125 0.125 0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.   ]\n",
      "  Doc 2: [0.  0.1 0.  0.1 0.1 0.  0.1 0.  0.  0.  0.1 0.  0.  0.  0.2 0.1 0.  0.\n",
      " 0.  0.1 0.  0.  0.  0.  0.1 0.  0.  0.  0.  0. ]\n",
      "  Doc 3: [0.    0.167 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.167\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.167 0.    0.    0.167\n",
      " 0.    0.167 0.167 0.    0.    0.   ]\n",
      "  Doc 4: [0.    0.    0.    0.    0.    0.143 0.    0.143 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.143 0.    0.    0.143 0.    0.143 0.    0.\n",
      " 0.    0.    0.    0.    0.143 0.143]\n",
      "  Doc 5: [0.125 0.125 0.125 0.    0.    0.    0.    0.    0.    0.125 0.    0.\n",
      " 0.    0.    0.    0.125 0.    0.    0.125 0.    0.    0.    0.125 0.\n",
      " 0.    0.    0.    0.125 0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# Normalize by dividing each count by the total count in that document\n",
    "normalized_count = count_array / count_array.sum(axis=1, keepdims=True)\n",
    "\n",
    "print(\"Normalized Count Occurrence Matrix:\")\n",
    "print(f\"  Shape: {normalized_count.shape}\")\n",
    "print(\"\\n  Document-Term Matrix (Normalized):\")\n",
    "for i, row in enumerate(normalized_count):\n",
    "    print(f\"  Doc {i+1}: {np.round(row, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c5cfd",
   "metadata": {},
   "source": [
    "## 3. TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2337ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "  ['analysis', 'and', 'applications', 'are', 'artificial', 'capture', 'deep', 'embeddings', 'field', 'include', 'intelligence', 'involves', 'is', 'language', 'learning', 'machine', 'meaning', 'natural', 'nlp', 'of', 'processing', 'semantic', 'sentiment', 'stemming', 'subfields', 'text', 'tokenization', 'translation', 'word', 'words']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "  Shape: (5, 30)\n",
      "\n",
      "  Document-Term Matrix (TF-IDF scores):\n",
      "  Doc 1: [0.    0.    0.    0.    0.319 0.    0.    0.    0.395 0.    0.319 0.\n",
      " 0.395 0.395 0.    0.    0.    0.395 0.    0.265 0.319 0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.   ]\n",
      "  Doc 2: [0.    0.213 0.    0.319 0.257 0.    0.319 0.    0.    0.    0.257 0.\n",
      " 0.    0.    0.637 0.257 0.    0.    0.    0.213 0.    0.    0.    0.\n",
      " 0.319 0.    0.    0.    0.    0.   ]\n",
      "  Doc 3: [0.    0.297 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.443\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.357 0.    0.    0.443\n",
      " 0.    0.443 0.443 0.    0.    0.   ]\n",
      "  Doc 4: [0.    0.    0.    0.    0.    0.394 0.    0.394 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.394 0.    0.    0.264 0.    0.394 0.    0.\n",
      " 0.    0.    0.    0.    0.394 0.394]\n",
      "  Doc 5: [0.375 0.251 0.375 0.    0.    0.    0.    0.    0.    0.375 0.    0.\n",
      " 0.    0.    0.    0.303 0.    0.    0.375 0.    0.    0.    0.375 0.\n",
      " 0.    0.    0.    0.375 0.    0.   ]\n",
      "\n",
      "Top 3 TF-IDF Terms per Document:\n",
      "  Doc 1: [('natural', np.float64(0.395)), ('is', np.float64(0.395)), ('language', np.float64(0.395))]\n",
      "  Doc 2: [('learning', np.float64(0.637)), ('subfields', np.float64(0.319)), ('deep', np.float64(0.319))]\n",
      "  Doc 3: [('text', np.float64(0.443)), ('tokenization', np.float64(0.443)), ('stemming', np.float64(0.443))]\n",
      "  Doc 4: [('words', np.float64(0.394)), ('word', np.float64(0.394)), ('meaning', np.float64(0.394))]\n",
      "  Doc 5: [('translation', np.float64(0.375)), ('sentiment', np.float64(0.375)), ('nlp', np.float64(0.375))]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"  {list(tfidf_vocabulary)}\")\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "print(f\"  Shape: {tfidf_array.shape}\")\n",
    "print(\"\\n  Document-Term Matrix (TF-IDF scores):\")\n",
    "for i, row in enumerate(tfidf_array):\n",
    "    print(f\"  Doc {i+1}: {np.round(row, 3)}\")\n",
    "\n",
    "# Show top TF-IDF terms per document\n",
    "print(\"\\nTop 3 TF-IDF Terms per Document:\")\n",
    "for i, row in enumerate(tfidf_array):\n",
    "    top_indices = row.argsort()[-3:][::-1]\n",
    "    top_terms = [(tfidf_vocabulary[idx], round(row[idx], 3)) for idx in top_indices if row[idx] > 0]\n",
    "    print(f\"  Doc {i+1}: {top_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902e1d8",
   "metadata": {},
   "source": [
    "## 4. Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42998732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Corpus:\n",
      "  Doc 1: ['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', '.']\n",
      "  Doc 2: ['machine', 'learning', 'and', 'deep', 'learning', 'are', 'subfields', 'of', 'artificial', 'intelligence', '.']\n",
      "  Doc 3: ['text', 'processing', 'involves', 'tokenization', 'and', 'stemming', '.']\n",
      "  Doc 4: ['word', 'embeddings', 'capture', 'semantic', 'meaning', 'of', 'words', '.']\n",
      "  Doc 5: ['nlp', 'applications', 'include', 'sentiment', 'analysis', 'and', 'machine', 'translation', '.']\n",
      "\n",
      "Word2Vec Model Info:\n",
      "  Vocabulary Size: 32\n",
      "  Embedding Dimension: 100\n",
      "\n",
      "Sample Word Embeddings (first 10 dimensions):\n",
      "  'learning': [-0.008  0.01   0.    -0.001  0.004 -0.005  0.003  0.008  0.005 -0.008]\n",
      "  'intelligence': [-0.009  0.002 -0.    -0.009 -0.01  -0.002  0.005  0.004 -0.007 -0.008]\n",
      "  'processing': [ 0.008 -0.004  0.009  0.009 -0.005 -0.     0.005 -0.003 -0.006 -0.007]\n",
      "  'words': [ 0.008  0.009  0.001 -0.008  0.008 -0.004  0.006  0.005  0.009 -0.01 ]\n",
      "\n",
      "Most Similar Words:\n",
      "  Similar to 'learning': [('words', 0.19617809355258942), ('intelligence', 0.15177428722381592), ('and', 0.09918354451656342)]\n",
      "  Similar to 'intelligence': [('language', 0.19019527733325958), ('include', 0.18686828017234802), ('learning', 0.15177427232265472)]\n",
      "  Similar to 'processing': [('word', 0.3552244007587433), ('semantic', 0.1990789920091629), ('.', 0.15627248585224152)]\n",
      "\n",
      "Word2Vec model saved as 'word2vec_model.model'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus for Word2Vec\n",
    "tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "print(\"Tokenized Corpus:\")\n",
    "for i, tokens in enumerate(tokenized_corpus):\n",
    "    print(f\"  Doc {i+1}: {tokens}\")\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,  # Embedding dimension\n",
    "    window=5,         # Context window\n",
    "    min_count=1,      # Minimum word count\n",
    "    sg=0,             # 0=CBOW, 1=Skip-gram\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "print(\"\\n Word2Vec Model Info:\")\n",
    "print(f\"  Vocabulary Size: {len(word2vec_model.wv)}\")\n",
    "print(f\"  Embedding Dimension: {word2vec_model.wv.vector_size}\")\n",
    "\n",
    "# Show embeddings for some words\n",
    "sample_words = [\"learning\", \"intelligence\", \"processing\", \"words\"]\n",
    "print(\"\\nSample Word Embeddings (first 10 dimensions):\")\n",
    "for word in sample_words:\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding = word2vec_model.wv[word][:10]\n",
    "        print(f\"  '{word}': {np.round(embedding, 3)}\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nMost Similar Words:\")\n",
    "similar_words_list = [(\"learning\", 3), (\"intelligence\", 3), (\"processing\", 3)]\n",
    "for word, topn in similar_words_list:\n",
    "    if word in word2vec_model.wv:\n",
    "        similar = word2vec_model.wv.most_similar(word, topn=topn)\n",
    "        print(f\"  Similar to '{word}': {similar}\")\n",
    "\n",
    "# Save Word2Vec model\n",
    "word2vec_model.save(\"word2vec_model.model\")\n",
    "print(\"\\nWord2Vec model saved as 'word2vec_model.model'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
