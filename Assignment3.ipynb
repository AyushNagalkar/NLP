{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6aed29",
   "metadata": {},
   "source": [
    "# Assignment 3: Text Cleaning, Lemmatization, Stop Words, Label Encoding, TF-IDF\n",
    "\n",
    "This notebook demonstrates:\n",
    "- **Text Cleaning** - Remove noise from text\n",
    "- **Lemmatization** - Convert words to base form\n",
    "- **Stop Words Removal** - Remove common words\n",
    "- **Label Encoding** - Encode categorical labels\n",
    "- **TF-IDF Representations** - Create TF-IDF features\n",
    "- **Save Outputs** - Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fa61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b8fde",
   "metadata": {},
   "source": [
    "## Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fb4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA:\n",
      "  [positive] The movie was AMAZING!!! I loved it so much <3 :)\n",
      "  [negative] This product is terrible... worst purchase ever!!! :/\n",
      "  [positive] Great service, very helpful staff & quick delivery.\n",
      "  [negative] Disappointed with the quality. Not worth the price...\n",
      "  [positive] Absolutely fantastic experience! Would recommend 10/10\n",
      "  [negative] Poor customer support, waited 2 hours on hold :(\n",
      "  [positive] Best restaurant in town! The food was delicious.\n",
      "  [negative] Never buying from this company again. Horrible!!!\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        \"The movie was AMAZING!!! I loved it so much <3 :)\",\n",
    "        \"This product is terrible... worst purchase ever!!! :/\",\n",
    "        \"Great service, very helpful staff & quick delivery.\",\n",
    "        \"Disappointed with the quality. Not worth the price...\",\n",
    "        \"Absolutely fantastic experience! Would recommend 10/10\",\n",
    "        \"Poor customer support, waited 2 hours on hold :(\",\n",
    "        \"Best restaurant in town! The food was delicious.\",\n",
    "        \"Never buying from this company again. Horrible!!!\"\n",
    "    ],\n",
    "    'category': ['positive', 'negative', 'positive', 'negative', \n",
    "                 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"ORIGINAL DATA:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"  [{row['category']:8}] {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea46be2",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e26fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      "  Original: The movie was AMAZING!!! I loved it so much <3 :)\n",
      "  Cleaned:  the movie was amazing i loved it so much\n",
      "\n",
      "  Original: This product is terrible... worst purchase ever!!! :/\n",
      "  Cleaned:  this product is terrible worst purchase ever\n",
      "\n",
      "  Original: Great service, very helpful staff & quick delivery.\n",
      "  Cleaned:  great service very helpful staff quick delivery\n",
      "\n",
      "  Original: Disappointed with the quality. Not worth the price...\n",
      "  Cleaned:  disappointed with the quality not worth the price\n",
      "\n",
      "  Original: Absolutely fantastic experience! Would recommend 10/10\n",
      "  Cleaned:  absolutely fantastic experience would recommend\n",
      "\n",
      "  Original: Poor customer support, waited 2 hours on hold :(\n",
      "  Cleaned:  poor customer support waited hours on hold\n",
      "\n",
      "  Original: Best restaurant in town! The food was delicious.\n",
      "  Cleaned:  best restaurant in town the food was delicious\n",
      "\n",
      "  Original: Never buying from this company again. Horrible!!!\n",
      "  Cleaned:  never buying from this company again horrible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing noise\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Cleaned Text:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"  Original: {row['text']}\")\n",
    "    print(f\"  Cleaned:  {row['cleaned_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9bcf2",
   "metadata": {},
   "source": [
    "## 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e125975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Text:\n",
      "  Before: the movie was amazing i loved it so much\n",
      "  After:  the movie be amaze i love it so much\n",
      "\n",
      "  Before: this product is terrible worst purchase ever\n",
      "  After:  this product be terrible worst purchase ever\n",
      "\n",
      "  Before: great service very helpful staff quick delivery\n",
      "  After:  great service very helpful staff quick delivery\n",
      "\n",
      "  Before: disappointed with the quality not worth the price\n",
      "  After:  disappoint with the quality not worth the price\n",
      "\n",
      "  Before: absolutely fantastic experience would recommend\n",
      "  After:  absolutely fantastic experience would recommend\n",
      "\n",
      "  Before: poor customer support waited hours on hold\n",
      "  After:  poor customer support wait hours on hold\n",
      "\n",
      "  Before: best restaurant in town the food was delicious\n",
      "  After:  best restaurant in town the food be delicious\n",
      "\n",
      "  Before: never buying from this company again horrible\n",
      "  After:  never buy from this company again horrible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "print(\"Lemmatized Text:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"  Before: {row['cleaned_text']}\")\n",
    "    print(f\"  After:  {row['lemmatized_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38ee15",
   "metadata": {},
   "source": [
    "## 3. Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4f09a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 198\n",
      "Sample stop words: ['an', 'aren', \"should've\", 'their', 'too', 'few', 'than', \"they've\", 'above', 'up', 'both', 'won', 'where', 'o', 'such', 'had', \"didn't\", \"hasn't\", 'him', 'did']\n",
      "\n",
      "Text after Stop Words Removal:\n",
      "  Before: the movie be amaze i love it so much\n",
      "  After:  movie amaze love much\n",
      "\n",
      "  Before: this product be terrible worst purchase ever\n",
      "  After:  product terrible worst purchase ever\n",
      "\n",
      "  Before: great service very helpful staff quick delivery\n",
      "  After:  great service helpful staff quick delivery\n",
      "\n",
      "  Before: disappoint with the quality not worth the price\n",
      "  After:  disappoint quality worth price\n",
      "\n",
      "  Before: absolutely fantastic experience would recommend\n",
      "  After:  absolutely fantastic experience would recommend\n",
      "\n",
      "  Before: poor customer support wait hours on hold\n",
      "  After:  poor customer support wait hours hold\n",
      "\n",
      "  Before: best restaurant in town the food be delicious\n",
      "  After:  best restaurant town food delicious\n",
      "\n",
      "  Before: never buy from this company again horrible\n",
      "  After:  never buy company horrible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Number of stop words: {len(stop_words)}\")\n",
    "print(f\"Sample stop words: {list(stop_words)[:20]}\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stop words from text\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df['no_stopwords_text'] = df['lemmatized_text'].apply(remove_stopwords)\n",
    "\n",
    "print(\"\\nText after Stop Words Removal:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"  Before: {row['lemmatized_text']}\")\n",
    "    print(f\"  After:  {row['no_stopwords_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94840a",
   "metadata": {},
   "source": [
    "## 4. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867c66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding:\n",
      "  Classes: ['negative' 'positive']\n",
      "  Mapping: {'negative': np.int64(0), 'positive': np.int64(1)}\n",
      "\n",
      "  Category -> Encoded Label:\n",
      "  positive   -> 1\n",
      "  negative   -> 0\n",
      "  positive   -> 1\n",
      "  negative   -> 0\n",
      "  positive   -> 1\n",
      "  negative   -> 0\n",
      "  positive   -> 1\n",
      "  negative   -> 0\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "print(\"Label Encoding:\")\n",
    "print(f\"  Classes: {label_encoder.classes_}\")\n",
    "print(f\"  Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "print(\"\\n  Category -> Encoded Label:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"  {row['category']:10} -> {row['encoded_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95c6a4",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ed8c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "  ['absolutely', 'amaze', 'best', 'buy', 'company', 'customer', 'delicious', 'delivery', 'disappoint', 'ever', 'experience', 'fantastic', 'food', 'great', 'helpful', 'hold', 'horrible', 'hours', 'love', 'movie', 'much', 'never', 'poor', 'price', 'product', 'purchase', 'quality', 'quick', 'recommend', 'restaurant', 'service', 'staff', 'support', 'terrible', 'town', 'wait', 'worst', 'worth', 'would']\n",
      "\n",
      "TF-IDF Matrix Shape: (8, 39)\n",
      "\n",
      "TF-IDF Representations (showing top 5 terms per document):\n",
      "  Doc 1: [('much', np.float64(0.5)), ('love', np.float64(0.5)), ('movie', np.float64(0.5)), ('amaze', np.float64(0.5))]\n",
      "  Doc 2: [('worst', np.float64(0.447)), ('terrible', np.float64(0.447)), ('purchase', np.float64(0.447)), ('product', np.float64(0.447)), ('ever', np.float64(0.447))]\n",
      "  Doc 3: [('quick', np.float64(0.408)), ('delivery', np.float64(0.408)), ('great', np.float64(0.408)), ('helpful', np.float64(0.408)), ('staff', np.float64(0.408))]\n",
      "  Doc 4: [('worth', np.float64(0.5)), ('price', np.float64(0.5)), ('quality', np.float64(0.5)), ('disappoint', np.float64(0.5))]\n",
      "  Doc 5: [('would', np.float64(0.447)), ('absolutely', np.float64(0.447)), ('recommend', np.float64(0.447)), ('experience', np.float64(0.447)), ('fantastic', np.float64(0.447))]\n",
      "  Doc 6: [('wait', np.float64(0.408)), ('support', np.float64(0.408)), ('hours', np.float64(0.408)), ('hold', np.float64(0.408)), ('customer', np.float64(0.408))]\n",
      "  Doc 7: [('town', np.float64(0.447)), ('best', np.float64(0.447)), ('restaurant', np.float64(0.447)), ('food', np.float64(0.447)), ('delicious', np.float64(0.447))]\n",
      "  Doc 8: [('company', np.float64(0.5)), ('never', np.float64(0.5)), ('horrible', np.float64(0.5)), ('buy', np.float64(0.5))]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['no_stopwords_text'])\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"  {list(vocabulary)}\")\n",
    "\n",
    "print(f\"\\nTF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"\\nTF-IDF Representations (showing top 5 terms per document):\")\n",
    "for i, row in enumerate(tfidf_array):\n",
    "    top_indices = row.argsort()[-5:][::-1]\n",
    "    top_terms = [(vocabulary[idx], round(row[idx], 3)) for idx in top_indices if row[idx] > 0]\n",
    "    print(f\"  Doc {i+1}: {top_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098954b",
   "metadata": {},
   "source": [
    "## 6. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb3f275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to 'processed_data.csv'\n",
      "TF-IDF matrix saved to 'tfidf_matrix.csv'\n",
      "Label encoder classes saved to 'label_encoder_classes.npy'\n",
      "Vocabulary saved to 'vocabulary.npy'\n",
      "\n",
      "FINAL PROCESSED DATAFRAME:\n",
      "                                                text  category  \\\n",
      "0  The movie was AMAZING!!! I loved it so much <3 :)  positive   \n",
      "1  This product is terrible... worst purchase eve...  negative   \n",
      "2  Great service, very helpful staff & quick deli...  positive   \n",
      "3  Disappointed with the quality. Not worth the p...  negative   \n",
      "4  Absolutely fantastic experience! Would recomme...  positive   \n",
      "5   Poor customer support, waited 2 hours on hold :(  negative   \n",
      "6   Best restaurant in town! The food was delicious.  positive   \n",
      "7  Never buying from this company again. Horrible!!!  negative   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0           the movie was amazing i loved it so much   \n",
      "1       this product is terrible worst purchase ever   \n",
      "2    great service very helpful staff quick delivery   \n",
      "3  disappointed with the quality not worth the price   \n",
      "4    absolutely fantastic experience would recommend   \n",
      "5         poor customer support waited hours on hold   \n",
      "6     best restaurant in town the food was delicious   \n",
      "7      never buying from this company again horrible   \n",
      "\n",
      "                                   lemmatized_text  \\\n",
      "0             the movie be amaze i love it so much   \n",
      "1     this product be terrible worst purchase ever   \n",
      "2  great service very helpful staff quick delivery   \n",
      "3  disappoint with the quality not worth the price   \n",
      "4  absolutely fantastic experience would recommend   \n",
      "5         poor customer support wait hours on hold   \n",
      "6    best restaurant in town the food be delicious   \n",
      "7       never buy from this company again horrible   \n",
      "\n",
      "                                 no_stopwords_text  encoded_label  \n",
      "0                            movie amaze love much              1  \n",
      "1             product terrible worst purchase ever              0  \n",
      "2       great service helpful staff quick delivery              1  \n",
      "3                   disappoint quality worth price              0  \n",
      "4  absolutely fantastic experience would recommend              1  \n",
      "5            poor customer support wait hours hold              0  \n",
      "6              best restaurant town food delicious              1  \n",
      "7                       never buy company horrible              0  \n"
     ]
    }
   ],
   "source": [
    "# Save processed dataframe to CSV\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "print(\"Processed data saved to 'processed_data.csv'\")\n",
    "\n",
    "# Save TF-IDF matrix to CSV\n",
    "tfidf_df = pd.DataFrame(tfidf_array, columns=vocabulary)\n",
    "tfidf_df.to_csv('tfidf_matrix.csv', index=False)\n",
    "print(\"TF-IDF matrix saved to 'tfidf_matrix.csv'\")\n",
    "\n",
    "# Save label encoder classes\n",
    "np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "print(\"Label encoder classes saved to 'label_encoder_classes.npy'\")\n",
    "\n",
    "# Save vocabulary\n",
    "np.save('vocabulary.npy', vocabulary)\n",
    "print(\"Vocabulary saved to 'vocabulary.npy'\")\n",
    "\n",
    "# Display final processed dataframe\n",
    "print(\"\\nFINAL PROCESSED DATAFRAME:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
